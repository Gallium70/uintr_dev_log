# 用户态中断 开发日志

## 2023.4.20

通过 trace 观察发现，由于某种“刻舟求剑”的原因——用户程序获取当前上下文（主要是 hart 编号）和执行 PLIC complete 操作之间，可能发生调度切换——导致 complete 操作失效，外设中断被永久屏蔽，因而中断模式驱动的效率下限极低。治标的手段是让用户程序连续执行两次 complete 操作，治本的手段需要内核给用户程序分配一个地址固定的虚拟 PLIC 上下文，在每次调度切换时在页表中修改该虚拟地址到 PLIC 物理上下文地址的映射。

“治标”之后中断驱动的性能下限大幅提升，双工模式下波动区间为 279K-299K 。

[commit 59bec68e](https://github.com/duskmoon314/rCore-N/commit/59bec68efa8b93466fecf2ad93e814f09ef9441b)

## 2023.4.13

调整了一下流控流程，接收方每收到 `PULSE_WIDTH` 个字符，就翻转一下 RTS ，发送方捕捉 CTS 上的双边沿统计接收方接收的字符数量，控制自己已经发送的字符数量和接收数量之差不超过 FIFO 深度。这样可以近似实现双缓冲的效果，提高通信速率。另外发现 trace 对性能影响比较大，目前默认关闭。

调整之后，轮询单工带宽达到 584K ，双工 443K ，中断单工 416K，双工 292K ，比之前有大幅提升，但中断速率还是偏低。

将波特率降低到 1.25M 测试，此时轮询双工 118K ，中断 40K ，轮询的带宽利用率上升，而中断的利用率下降，应该是中断驱动的额外开销还是较大。

[commit 2c48960](https://github.com/duskmoon314/rCore-N/commit/2c48960320f1667aac72c78e76a457b440a6ddd6)

## 2023.4.6

完成中断模式驱动的流控功能，但是吞吐量不是很稳定，从25K到156K波动。峰值大约为轮询模式的一半。

异步中断模式的驱动行为比较奇怪，如果两边都同时收发，在大概三四个周期之后就会卡住，没有可以 wake 的 future ，但是 executor 里也不会再加新的 future ；如果一边收一边发，那么很快堆分配器会报错，怀疑是创建了过多的 future 或者存在内存泄漏，需要继续调试。

[commit 7cfd3f9](https://github.com/duskmoon314/rCore-N/commit/7cfd3f922c2bbd3626d05cb592fee9d6e15bda96)

## 2023.3.30

开启了串口硬件流控，将两个 16550 的 RTS/CTS 对接在一起，参考 [UART串口流控制（Flow control）](https://blog.csdn.net/qq_42992084/article/details/104761474) 。注意 RTS/CTS 只传输信号，不会影响串口内的发射器和接收器工作，需要驱动软件自行协调。

设计的通信流程如下：

1. 接收方拉高 RTS，进入就绪状态；
2. 发送方在就绪状态读到 CTS 上升沿 (`MSR.DCTS && MSR.CTS`) ，进入正在发送状态；
3. 发送方发送一批数据，进入就绪状态，回到 2 ；
4. 接收方初次收到数据后拉低 RTS，进入正在接收状态；
5. 接收方收到完整一批数据后，回到 1 ；

每批数据的大小不超过硬件 FIFO 的尺寸即 16 字节。

在这种设计和 6.25M 波特率下，轮询模式驱动可以达到 330 - 360 KB/s 吞吐量，且完全不丢数据。中断模式驱动还在修改中。

[commit e2f8266](https://github.com/duskmoon314/rCore-N/commit/e2f8266b26b70e4069cca0b3b9386b1917c36f8b)

## 2023.3.23

分析了丢数据的原因：乒乓测试设计的发送条件是，自己的发送FIFO为空，且从对方收到了一批数据。波特率提升之后，发送FIFO会很快清空，这样有一方可能会快速连续发出去两批数据。

- 如果一批的数据量大于接收FIFO的一半，就会把对方的接收FIFO挤溢出了。（当然也可能中间对方读取走了一部分，但是积累下来还是可能溢出）
- 如果一批数据恰好是FIFO的一半，那连着发送两批数据恰好占满FIFO而不溢出，等到能第三次发送时，对方一定已经从FIFO读取走了一批数据，这样就不会溢出。

![fifo_overrun](fifo_overrun.svg)

构造 16 个字节的消息，在两个串口之间循环发送（收到数据就立刻发出），缓冲区不会溢出，基本确认 FIFO 不存在硬件缺陷。

16550 无法检查缓冲区中的数据数量，稳妥的操作方式是每次读取前先检查 lcr.dr 状态，但这样似乎会影响接收效率？仅 70K/s 。正在尝试改成利用 Received Data Available 中断，配合设置接收 FIFO 触发水平，以及轮询 iid 寄存器，减少对 lcr.dr 的访问次数。

正在往 Qemu 7.0-uintr 中添加用户态外部中断。

## 2023.3.16

测试FIFO+轮询的乒乓驱动，每次发一定数量的字符，再等待收到同样数量的字符，再进行下一轮操作。

行为比较怪异：如果每轮发送和接收的不超过8个字符，那么不会出现错误，且吞吐量基本线性增长，最高可达400K；

一旦超过8个，就会出现丢数据的情况。仿佛收发的FIFO深度只有8，但几份数据手册上都写的是16。

尝试了若干参数组合，现象如下：

1. 每次尝试发送的字符越多，出现错误越早。
2. 波特率越低，出现错误越早。
3. 系统的调度周期会影响出错位置的离散度（固定每周期收发12字符）：

| 调度周期 | 首次出错位置分布 |
|- | - |
| 0.01s | 300 - 2.5k |
| 0.1s | 800 - 50k |
| 1s | 30 - 400k |

4. 在驱动里限制发送（每次都等到 LCR.THRE 有效再发送），相当于禁用发送 FIFO ，没什么效果，还是会出错。

目前看起来影响比较大的可能是调度，之后也要看一下生成的IP核的硬件代码，确认一下FIFO深度到底是不是16。


## 2022.3.9

在fpga板上测试，发现两个问题：

1. 即使是轮询模式，收发也不对等，收的数量仅有发的1/3左右，且效率低于之前的版本
2. 中断模式收不到数据

这两个问题与异步驱动无关，怀疑与PAC库有关。再次梳理初始化流程，加入了更多的寄存器复位操作，重新调整缓冲区参数后，轮询模式效率有所提升，但仍然低于之前。

关于中断，发现是PAC库对于寄存器提供write和modify两个参数，前者会覆盖整个寄存器的内容（类似csrw指令），后者只会更改指定的位域（类似csrc/csrs）。之前用write操作16550的IER寄存器，会在开启读取中断的同时关掉写入中断，反之亦然。改为modify之后可以正常收发，但效率同样较低。

中断模式修改后，异步驱动可以收发数据，但吞吐量异常低，仅有1KB/s左右，原因仍在排查。

下周：

先做乒乓收发，解决轮询和中断丢数据问题，低于5%，确定三个模式都正常工作，然后再考虑异步驱动的效率

日志放进仓库

## 2022.3.2

异步驱动

先前的故障：基于用户态中断的异步串口驱动只发不收，发的字节数量恰好为设定的软件缓冲区的大小
 
分析：从收发字节数量入手，推测可能是读的异步任务跑飞了，对端把缓冲区塞满之后就无法工作。

插入调试输出，观察到在驱动收到中断后，读/写任务的waker丢失，将缓冲区调小之后很快就没有输出，确认程序将驱动的缓冲区填满一次之后就停止工作。
 
推测故障原因：

之前在测试负载中，构造了一个executor，往其中加入了一个读任务和一个写任务，当这两个任务均运行完成（自身任务的缓冲区存满或耗尽）时，再向executor加入一个读任务和写任务。但如果收发速率稍微不均衡，可能在两边读任务都先跑完，而写任务将对端缓冲区填满之后，不会再消耗，两个测试任务出现死锁，直到测试超时。
 
修改方法：

读任务和写任务的创建解耦，目前为读写创建了两个executor
 
修改后的效果

写入数量远超驱动缓冲区，读写任务会持续运行到测试程序超时。接收量仍然偏少，推测在qemu中设备后端的缓冲区较大，且处理速度较快，写入总会完成，读任务可能被饥饿。后续在硬件平台上测试。
